{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7167/2053734199.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers = pd.read_csv('reddit_answers_big.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_votes= df_answers.groupby('q_id')['votes'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_answers = df_answers.loc[df_top_votes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_answers.rename(columns = {'text':'answers'}, inplace = True)\n",
    "df_top_answers.rename(columns = {'q_id':'id'}, inplace = True)\n",
    "df_top_answers.rename(columns = {'votes':'answer_votes'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('reddit_questions.csv', sep = ';')\n",
    "df_questions.rename(columns = {'votes':'question_votes'}, inplace = True)\n",
    "df_questions.rename(columns = {'text':'question'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers.rename(columns = {'text':'question'}, inplace = True)\n",
    "df_answers.rename(columns = {'votes':'question_votes'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_questions.merge(df_top_answers, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns = ['timestamp', 'datetime', 'Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df= merged_df.reindex(columns = ['id', 'question',  'answers', 'question_votes', 'answer_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.sort_values(by = 'answer_votes', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df  = merged_df [:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>question_votes</th>\n",
       "      <th>answer_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75274</th>\n",
       "      <td>fkzaca</td>\n",
       "      <td>What is something that has aged well?</td>\n",
       "      <td>The word cool</td>\n",
       "      <td>66093</td>\n",
       "      <td>99398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167081</th>\n",
       "      <td>a0a4cd</td>\n",
       "      <td>What's the most amazing thing about the universe?</td>\n",
       "      <td>It must be true that either  It didn't exist, ...</td>\n",
       "      <td>81862</td>\n",
       "      <td>86042.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140939</th>\n",
       "      <td>d0jjc2</td>\n",
       "      <td>The 2010's decade will be over in 4 months. Wh...</td>\n",
       "      <td>The social media explosion</td>\n",
       "      <td>113254</td>\n",
       "      <td>85936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128868</th>\n",
       "      <td>aqf3bi</td>\n",
       "      <td>You are offered $1,000,000 USD if you can hide...</td>\n",
       "      <td>Easy, ask the CIA to hold them...those two don...</td>\n",
       "      <td>81908</td>\n",
       "      <td>85693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18363</th>\n",
       "      <td>bvdaci</td>\n",
       "      <td>What's classy if you're rich but trashy if you...</td>\n",
       "      <td>The most expensive thing you own is a really o...</td>\n",
       "      <td>66102</td>\n",
       "      <td>85568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139296</th>\n",
       "      <td>9w884k</td>\n",
       "      <td>What's your \"\"thank god that's over with and I...</td>\n",
       "      <td>Finding a first job. It was at least 90% luck,...</td>\n",
       "      <td>33470</td>\n",
       "      <td>44574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32384</th>\n",
       "      <td>82630j</td>\n",
       "      <td>What profession was once highly respected, but...</td>\n",
       "      <td>Bank teller used to be a foot in the door to t...</td>\n",
       "      <td>46027</td>\n",
       "      <td>44549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141294</th>\n",
       "      <td>a2zwpp</td>\n",
       "      <td>What's a rule that was implemented somewhere, ...</td>\n",
       "      <td>My company has a strict no alcohol policy. You...</td>\n",
       "      <td>52721</td>\n",
       "      <td>44504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155069</th>\n",
       "      <td>cjr8nm</td>\n",
       "      <td>What folklore creature do you think really exi...</td>\n",
       "      <td>The Kraken is almost definitely an extra thicc...</td>\n",
       "      <td>51781</td>\n",
       "      <td>44499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112570</th>\n",
       "      <td>hy77f0</td>\n",
       "      <td>[Serious] People of reddit who have gone throu...</td>\n",
       "      <td>I felt a pop in my back. It was actually a ver...</td>\n",
       "      <td>90514</td>\n",
       "      <td>44491.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           question  \\\n",
       "75274   fkzaca              What is something that has aged well?   \n",
       "167081  a0a4cd  What's the most amazing thing about the universe?   \n",
       "140939  d0jjc2  The 2010's decade will be over in 4 months. Wh...   \n",
       "128868  aqf3bi  You are offered $1,000,000 USD if you can hide...   \n",
       "18363   bvdaci  What's classy if you're rich but trashy if you...   \n",
       "...        ...                                                ...   \n",
       "139296  9w884k  What's your \"\"thank god that's over with and I...   \n",
       "32384   82630j  What profession was once highly respected, but...   \n",
       "141294  a2zwpp  What's a rule that was implemented somewhere, ...   \n",
       "155069  cjr8nm  What folklore creature do you think really exi...   \n",
       "112570  hy77f0  [Serious] People of reddit who have gone throu...   \n",
       "\n",
       "                                                  answers  question_votes  \\\n",
       "75274                                       The word cool           66093   \n",
       "167081  It must be true that either  It didn't exist, ...           81862   \n",
       "140939                         The social media explosion          113254   \n",
       "128868  Easy, ask the CIA to hold them...those two don...           81908   \n",
       "18363   The most expensive thing you own is a really o...           66102   \n",
       "...                                                   ...             ...   \n",
       "139296  Finding a first job. It was at least 90% luck,...           33470   \n",
       "32384   Bank teller used to be a foot in the door to t...           46027   \n",
       "141294  My company has a strict no alcohol policy. You...           52721   \n",
       "155069  The Kraken is almost definitely an extra thicc...           51781   \n",
       "112570  I felt a pop in my back. It was actually a ver...           90514   \n",
       "\n",
       "        answer_votes  \n",
       "75274        99398.0  \n",
       "167081       86042.0  \n",
       "140939       85936.0  \n",
       "128868       85693.0  \n",
       "18363        85568.0  \n",
       "...              ...  \n",
       "139296       44574.0  \n",
       "32384        44549.0  \n",
       "141294       44504.0  \n",
       "155069       44499.0  \n",
       "112570       44491.0  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = merged_df['question'], merged_df['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_openai_format = [{\"messages\": [{'role': 'system', 'content': 'Marv is a factual chatbot and reddit expert who likes to answer with bullets'},\n",
    "                                  {'role': 'user', 'content': q},\n",
    "                                  {'role': 'assistant', 'content': a}]}\n",
    "                    for q, a in zip(questions, answers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Marv is a factual chatbot and reddit expert who likes to answer with bullets'},\n",
       "  {'role': 'user',\n",
       "   'content': 'You are offered $1,000,000 USD if you can hide a pair of car keys from the entirety of the FBI force for 7 days. Where do you hide the keys?'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Easy, ask the CIA to hold them...those two don't share shit.\"}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_openai_format[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.jsonl', 'w') as f:\n",
    "    for entry in qa_openai_format:\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 examples\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "data_path = \"training_data.jsonl\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    dataset= [json.loads(line) for line in f]\n",
    "    \n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-pqN0u7lBPpI82Bv4udlgpejD', bytes=235808, created_at=1713506896, filename='training_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.create(file=open(\"training_data.jsonl\", \"rb\"), purpose=\"fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file-pqN0u7lBPpI82Bv4udlgpejD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-i3343i8eFZ1lbBYoWKDJvL4w', created_at=1713507047, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-To0uUA7DnRus6HYSgl3vvG8G', result_files=[], seed=1240979456, status='validating_files', trained_tokens=None, training_file='file-pqN0u7lBPpI82Bv4udlgpejD', validation_file=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(training_file=\"file-pqN0u7lBPpI82Bv4udlgpejD\", model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-i3343i8eFZ1lbBYoWKDJvL4w', created_at=1713507047, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-To0uUA7DnRus6HYSgl3vvG8G', result_files=[], seed=1240979456, status='running', trained_tokens=None, training_file='file-pqN0u7lBPpI82Bv4udlgpejD', validation_file=None, integrations=[], user_provided_suffix=None)], object='list', has_more=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id= 'ftjob-i3343i8eFZ1lbBYoWKDJvL4w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-19 14:27:07 Step 564/1500: training loss=2.20\n",
      "2024-04-19 14:27:05 Step 563/1500: training loss=1.92\n",
      "2024-04-19 14:27:05 Step 562/1500: training loss=2.24\n",
      "2024-04-19 14:27:03 Step 561/1500: training loss=0.91\n",
      "2024-04-19 14:27:01 Step 560/1500: training loss=1.73\n",
      "2024-04-19 14:27:01 Step 559/1500: training loss=1.36\n",
      "2024-04-19 14:26:59 Step 558/1500: training loss=2.58\n",
      "2024-04-19 14:26:57 Step 557/1500: training loss=1.28\n",
      "2024-04-19 14:26:55 Step 556/1500: training loss=1.05\n",
      "2024-04-19 14:26:55 Step 555/1500: training loss=1.44\n",
      "2024-04-19 14:26:53 Step 554/1500: training loss=4.25\n",
      "2024-04-19 14:26:51 Step 553/1500: training loss=2.61\n",
      "2024-04-19 14:26:49 Step 552/1500: training loss=1.54\n",
      "2024-04-19 14:26:49 Step 551/1500: training loss=1.97\n",
      "2024-04-19 14:26:47 Step 550/1500: training loss=2.37\n",
      "2024-04-19 14:26:45 Step 549/1500: training loss=1.71\n",
      "2024-04-19 14:26:45 Step 548/1500: training loss=1.86\n",
      "2024-04-19 14:26:43 Step 547/1500: training loss=1.81\n",
      "2024-04-19 14:26:41 Step 546/1500: training loss=2.45\n",
      "2024-04-19 14:26:39 Step 545/1500: training loss=1.42\n",
      "2024-04-19 14:26:39 Step 544/1500: training loss=1.31\n",
      "2024-04-19 14:26:36 Step 543/1500: training loss=2.27\n",
      "2024-04-19 14:26:34 Step 542/1500: training loss=1.34\n",
      "2024-04-19 14:26:32 Step 541/1500: training loss=1.77\n",
      "2024-04-19 14:26:32 Step 540/1500: training loss=1.19\n",
      "2024-04-19 14:26:30 Step 539/1500: training loss=2.04\n",
      "2024-04-19 14:26:28 Step 538/1500: training loss=2.18\n",
      "2024-04-19 14:26:26 Step 537/1500: training loss=2.14\n",
      "2024-04-19 14:26:26 Step 536/1500: training loss=1.75\n",
      "2024-04-19 14:26:24 Step 535/1500: training loss=2.00\n",
      "2024-04-19 14:26:22 Step 534/1500: training loss=2.16\n",
      "2024-04-19 14:26:20 Step 533/1500: training loss=2.23\n",
      "2024-04-19 14:26:20 Step 532/1500: training loss=1.38\n",
      "2024-04-19 14:26:18 Step 531/1500: training loss=1.51\n",
      "2024-04-19 14:26:16 Step 530/1500: training loss=1.58\n",
      "2024-04-19 14:26:16 Step 529/1500: training loss=1.72\n",
      "2024-04-19 14:26:14 Step 528/1500: training loss=2.01\n",
      "2024-04-19 14:26:12 Step 527/1500: training loss=0.92\n",
      "2024-04-19 14:26:10 Step 526/1500: training loss=2.01\n",
      "2024-04-19 14:26:10 Step 525/1500: training loss=1.71\n",
      "2024-04-19 14:26:08 Step 524/1500: training loss=3.43\n",
      "2024-04-19 14:26:06 Step 523/1500: training loss=0.78\n",
      "2024-04-19 14:26:04 Step 522/1500: training loss=1.80\n",
      "2024-04-19 14:26:04 Step 521/1500: training loss=1.33\n",
      "2024-04-19 14:26:02 Step 520/1500: training loss=0.86\n",
      "2024-04-19 14:26:00 Step 519/1500: training loss=2.55\n",
      "2024-04-19 14:26:00 Step 518/1500: training loss=1.94\n",
      "2024-04-19 14:25:58 Step 517/1500: training loss=2.81\n",
      "2024-04-19 14:25:56 Step 516/1500: training loss=1.07\n",
      "2024-04-19 14:25:54 Step 515/1500: training loss=1.50\n",
      "2024-04-19 14:25:54 Step 514/1500: training loss=1.79\n",
      "2024-04-19 14:25:52 Step 513/1500: training loss=1.73\n",
      "2024-04-19 14:25:50 Step 512/1500: training loss=2.06\n",
      "2024-04-19 14:25:48 Step 511/1500: training loss=1.46\n",
      "2024-04-19 14:25:46 Step 510/1500: training loss=1.80\n",
      "2024-04-19 14:25:46 Step 509/1500: training loss=0.79\n",
      "2024-04-19 14:25:44 Step 508/1500: training loss=4.49\n",
      "2024-04-19 14:25:42 Step 507/1500: training loss=1.45\n",
      "2024-04-19 14:25:40 Step 506/1500: training loss=2.48\n",
      "2024-04-19 14:25:40 Step 505/1500: training loss=2.98\n",
      "2024-04-19 14:25:38 Step 504/1500: training loss=2.02\n",
      "2024-04-19 14:25:36 Step 503/1500: training loss=2.12\n",
      "2024-04-19 14:25:36 Step 502/1500: training loss=1.64\n",
      "2024-04-19 14:25:18 Step 501/1500: training loss=1.95\n",
      "2024-04-19 14:25:18 Step 500/1500: training loss=2.43\n",
      "2024-04-19 14:25:14 Step 499/1500: training loss=1.85\n",
      "2024-04-19 14:25:14 Step 498/1500: training loss=2.07\n",
      "2024-04-19 14:25:12 Step 497/1500: training loss=3.20\n",
      "2024-04-19 14:25:10 Step 496/1500: training loss=3.71\n",
      "2024-04-19 14:25:10 Step 495/1500: training loss=2.17\n",
      "2024-04-19 14:25:08 Step 494/1500: training loss=2.12\n",
      "2024-04-19 14:25:06 Step 493/1500: training loss=2.60\n",
      "2024-04-19 14:25:04 Step 492/1500: training loss=2.15\n",
      "2024-04-19 14:25:04 Step 491/1500: training loss=2.17\n",
      "2024-04-19 14:25:02 Step 490/1500: training loss=2.14\n",
      "2024-04-19 14:25:00 Step 489/1500: training loss=1.27\n",
      "2024-04-19 14:24:58 Step 488/1500: training loss=2.28\n",
      "2024-04-19 14:24:58 Step 487/1500: training loss=2.35\n",
      "2024-04-19 14:24:56 Step 486/1500: training loss=2.27\n",
      "2024-04-19 14:24:54 Step 485/1500: training loss=6.89\n",
      "2024-04-19 14:24:52 Step 484/1500: training loss=3.29\n",
      "2024-04-19 14:24:52 Step 483/1500: training loss=0.67\n",
      "2024-04-19 14:24:50 Step 482/1500: training loss=2.22\n",
      "2024-04-19 14:24:47 Step 481/1500: training loss=2.80\n",
      "2024-04-19 14:24:47 Step 480/1500: training loss=1.31\n",
      "2024-04-19 14:24:45 Step 479/1500: training loss=1.66\n",
      "2024-04-19 14:24:43 Step 478/1500: training loss=1.87\n",
      "2024-04-19 14:24:41 Step 477/1500: training loss=3.32\n",
      "2024-04-19 14:24:41 Step 476/1500: training loss=1.57\n",
      "2024-04-19 14:24:39 Step 475/1500: training loss=2.36\n",
      "2024-04-19 14:24:37 Step 474/1500: training loss=1.52\n",
      "2024-04-19 14:24:35 Step 473/1500: training loss=2.21\n",
      "2024-04-19 14:24:35 Step 472/1500: training loss=3.49\n",
      "2024-04-19 14:24:33 Step 471/1500: training loss=2.22\n",
      "2024-04-19 14:24:31 Step 470/1500: training loss=1.52\n",
      "2024-04-19 14:24:29 Step 469/1500: training loss=1.79\n",
      "2024-04-19 14:24:29 Step 468/1500: training loss=2.20\n",
      "2024-04-19 14:24:27 Step 467/1500: training loss=4.42\n",
      "2024-04-19 14:24:25 Step 466/1500: training loss=1.45\n",
      "2024-04-19 14:24:25 Step 465/1500: training loss=1.46\n",
      "2024-04-19 14:24:23 Step 464/1500: training loss=4.66\n",
      "2024-04-19 14:24:21 Step 463/1500: training loss=2.40\n",
      "2024-04-19 14:24:19 Step 462/1500: training loss=2.12\n",
      "2024-04-19 14:24:19 Step 461/1500: training loss=1.91\n",
      "2024-04-19 14:24:17 Step 460/1500: training loss=1.99\n",
      "2024-04-19 14:24:15 Step 459/1500: training loss=1.75\n",
      "2024-04-19 14:24:13 Step 458/1500: training loss=2.01\n",
      "2024-04-19 14:24:13 Step 457/1500: training loss=1.85\n",
      "2024-04-19 14:24:11 Step 456/1500: training loss=1.97\n",
      "2024-04-19 14:24:09 Step 455/1500: training loss=2.25\n",
      "2024-04-19 14:24:09 Step 454/1500: training loss=2.25\n",
      "2024-04-19 14:24:07 Step 453/1500: training loss=0.91\n",
      "2024-04-19 14:24:05 Step 452/1500: training loss=1.77\n",
      "2024-04-19 14:24:03 Step 451/1500: training loss=1.09\n",
      "2024-04-19 14:24:03 Step 450/1500: training loss=0.93\n",
      "2024-04-19 14:24:01 Step 449/1500: training loss=1.97\n",
      "2024-04-19 14:23:59 Step 448/1500: training loss=1.56\n",
      "2024-04-19 14:23:59 Step 447/1500: training loss=2.46\n",
      "2024-04-19 14:23:57 Step 446/1500: training loss=1.50\n",
      "2024-04-19 14:23:55 Step 445/1500: training loss=1.88\n",
      "2024-04-19 14:23:53 Step 444/1500: training loss=2.39\n",
      "2024-04-19 14:23:53 Step 443/1500: training loss=2.26\n",
      "2024-04-19 14:23:51 Step 442/1500: training loss=1.45\n",
      "2024-04-19 14:23:49 Step 441/1500: training loss=2.08\n",
      "2024-04-19 14:23:47 Step 440/1500: training loss=2.10\n",
      "2024-04-19 14:23:47 Step 439/1500: training loss=1.72\n",
      "2024-04-19 14:23:45 Step 438/1500: training loss=3.38\n",
      "2024-04-19 14:23:43 Step 437/1500: training loss=2.13\n",
      "2024-04-19 14:23:41 Step 436/1500: training loss=2.05\n",
      "2024-04-19 14:23:41 Step 435/1500: training loss=2.13\n",
      "2024-04-19 14:23:39 Step 434/1500: training loss=2.45\n",
      "2024-04-19 14:23:37 Step 433/1500: training loss=1.64\n",
      "2024-04-19 14:23:37 Step 432/1500: training loss=1.89\n",
      "2024-04-19 14:23:35 Step 431/1500: training loss=2.45\n",
      "2024-04-19 14:23:33 Step 430/1500: training loss=3.30\n",
      "2024-04-19 14:23:31 Step 429/1500: training loss=3.53\n",
      "2024-04-19 14:23:31 Step 428/1500: training loss=1.67\n",
      "2024-04-19 14:23:29 Step 427/1500: training loss=2.11\n",
      "2024-04-19 14:23:27 Step 426/1500: training loss=2.24\n",
      "2024-04-19 14:23:25 Step 425/1500: training loss=1.80\n",
      "2024-04-19 14:23:25 Step 424/1500: training loss=2.34\n",
      "2024-04-19 14:23:23 Step 423/1500: training loss=3.03\n",
      "2024-04-19 14:23:21 Step 422/1500: training loss=2.32\n",
      "2024-04-19 14:23:21 Step 421/1500: training loss=2.38\n",
      "2024-04-19 14:23:19 Step 420/1500: training loss=2.17\n",
      "2024-04-19 14:23:17 Step 419/1500: training loss=3.12\n",
      "2024-04-19 14:23:15 Step 418/1500: training loss=1.55\n",
      "2024-04-19 14:23:15 Step 417/1500: training loss=2.31\n",
      "2024-04-19 14:23:13 Step 416/1500: training loss=2.48\n",
      "2024-04-19 14:23:11 Step 415/1500: training loss=2.05\n",
      "2024-04-19 14:23:09 Step 414/1500: training loss=2.45\n",
      "2024-04-19 14:23:09 Step 413/1500: training loss=1.54\n",
      "2024-04-19 14:23:07 Step 412/1500: training loss=1.66\n",
      "2024-04-19 14:23:05 Step 411/1500: training loss=1.91\n",
      "2024-04-19 14:23:05 Step 410/1500: training loss=2.39\n",
      "2024-04-19 14:23:03 Step 409/1500: training loss=1.69\n",
      "2024-04-19 14:23:01 Step 408/1500: training loss=1.25\n",
      "2024-04-19 14:22:59 Step 407/1500: training loss=1.81\n",
      "2024-04-19 14:22:59 Step 406/1500: training loss=2.24\n",
      "2024-04-19 14:22:57 Step 405/1500: training loss=1.62\n",
      "2024-04-19 14:22:55 Step 404/1500: training loss=2.01\n",
      "2024-04-19 14:22:53 Step 403/1500: training loss=1.67\n",
      "2024-04-19 14:22:53 Step 402/1500: training loss=2.28\n",
      "2024-04-19 14:22:51 Step 401/1500: training loss=2.07\n",
      "2024-04-19 14:22:49 Step 400/1500: training loss=1.68\n",
      "2024-04-19 14:22:46 Step 399/1500: training loss=2.39\n",
      "2024-04-19 14:22:46 Step 398/1500: training loss=2.27\n",
      "2024-04-19 14:22:44 Step 397/1500: training loss=2.48\n",
      "2024-04-19 14:22:42 Step 396/1500: training loss=1.79\n",
      "2024-04-19 14:22:40 Step 395/1500: training loss=2.00\n",
      "2024-04-19 14:22:40 Step 394/1500: training loss=1.99\n",
      "2024-04-19 14:22:38 Step 393/1500: training loss=1.63\n",
      "2024-04-19 14:22:36 Step 392/1500: training loss=3.15\n",
      "2024-04-19 14:22:36 Step 391/1500: training loss=1.71\n",
      "2024-04-19 14:22:34 Step 390/1500: training loss=2.47\n",
      "2024-04-19 14:22:32 Step 389/1500: training loss=2.84\n",
      "2024-04-19 14:22:30 Step 388/1500: training loss=2.10\n",
      "2024-04-19 14:22:30 Step 387/1500: training loss=1.33\n",
      "2024-04-19 14:22:28 Step 386/1500: training loss=1.71\n",
      "2024-04-19 14:22:26 Step 385/1500: training loss=2.59\n",
      "2024-04-19 14:22:24 Step 384/1500: training loss=2.19\n",
      "2024-04-19 14:22:24 Step 383/1500: training loss=2.11\n",
      "2024-04-19 14:22:22 Step 382/1500: training loss=1.56\n",
      "2024-04-19 14:22:20 Step 381/1500: training loss=4.42\n",
      "2024-04-19 14:22:18 Step 380/1500: training loss=3.06\n",
      "2024-04-19 14:22:18 Step 379/1500: training loss=8.60\n",
      "2024-04-19 14:22:16 Step 378/1500: training loss=1.99\n",
      "2024-04-19 14:22:08 Step 377/1500: training loss=1.95\n",
      "2024-04-19 14:22:06 Step 376/1500: training loss=1.94\n",
      "2024-04-19 14:22:04 Step 375/1500: training loss=2.67\n",
      "2024-04-19 14:22:04 Step 374/1500: training loss=1.45\n",
      "2024-04-19 14:22:02 Step 373/1500: training loss=1.50\n",
      "2024-04-19 14:22:00 Step 372/1500: training loss=1.85\n",
      "2024-04-19 14:21:58 Step 371/1500: training loss=2.01\n",
      "2024-04-19 14:21:58 Step 370/1500: training loss=1.83\n",
      "2024-04-19 14:21:56 Step 369/1500: training loss=1.03\n",
      "2024-04-19 14:21:54 Step 368/1500: training loss=2.41\n",
      "2024-04-19 14:21:54 Step 367/1500: training loss=2.05\n",
      "2024-04-19 14:21:52 Step 366/1500: training loss=2.46\n",
      "2024-04-19 14:21:50 Step 365/1500: training loss=1.54\n",
      "2024-04-19 14:21:48 Step 364/1500: training loss=2.22\n",
      "2024-04-19 14:21:48 Step 363/1500: training loss=2.27\n",
      "2024-04-19 14:21:46 Step 362/1500: training loss=1.47\n",
      "2024-04-19 14:21:44 Step 361/1500: training loss=1.84\n",
      "2024-04-19 14:21:42 Step 360/1500: training loss=1.66\n",
      "2024-04-19 14:21:42 Step 359/1500: training loss=2.43\n",
      "2024-04-19 14:21:40 Step 358/1500: training loss=2.08\n",
      "2024-04-19 14:21:38 Step 357/1500: training loss=1.49\n",
      "2024-04-19 14:21:36 Step 356/1500: training loss=0.82\n",
      "2024-04-19 14:21:36 Step 355/1500: training loss=2.70\n",
      "2024-04-19 14:21:34 Step 354/1500: training loss=1.82\n",
      "2024-04-19 14:21:32 Step 353/1500: training loss=2.37\n",
      "2024-04-19 14:21:32 Step 352/1500: training loss=1.77\n",
      "2024-04-19 14:21:30 Step 351/1500: training loss=2.44\n",
      "2024-04-19 14:21:28 Step 350/1500: training loss=2.35\n",
      "2024-04-19 14:21:26 Step 349/1500: training loss=1.88\n",
      "2024-04-19 14:21:26 Step 348/1500: training loss=1.87\n",
      "2024-04-19 14:21:24 Step 347/1500: training loss=3.18\n",
      "2024-04-19 14:21:22 Step 346/1500: training loss=1.85\n",
      "2024-04-19 14:21:22 Step 345/1500: training loss=1.91\n",
      "2024-04-19 14:21:20 Step 344/1500: training loss=2.29\n",
      "2024-04-19 14:21:18 Step 343/1500: training loss=2.40\n",
      "2024-04-19 14:21:16 Step 342/1500: training loss=5.15\n",
      "2024-04-19 14:21:16 Step 341/1500: training loss=2.13\n",
      "2024-04-19 14:21:14 Step 340/1500: training loss=3.14\n",
      "2024-04-19 14:21:12 Step 339/1500: training loss=2.19\n",
      "2024-04-19 14:21:10 Step 338/1500: training loss=1.86\n",
      "2024-04-19 14:21:10 Step 337/1500: training loss=2.79\n",
      "2024-04-19 14:21:08 Step 336/1500: training loss=2.00\n",
      "2024-04-19 14:21:06 Step 335/1500: training loss=2.84\n",
      "2024-04-19 14:21:04 Step 334/1500: training loss=1.78\n",
      "2024-04-19 14:21:04 Step 333/1500: training loss=2.95\n",
      "2024-04-19 14:21:02 Step 332/1500: training loss=2.01\n",
      "2024-04-19 14:21:00 Step 331/1500: training loss=3.44\n",
      "2024-04-19 14:21:00 Step 330/1500: training loss=2.04\n",
      "2024-04-19 14:20:58 Step 329/1500: training loss=1.92\n",
      "2024-04-19 14:20:56 Step 328/1500: training loss=2.17\n",
      "2024-04-19 14:20:54 Step 327/1500: training loss=1.38\n",
      "2024-04-19 14:20:54 Step 326/1500: training loss=1.94\n",
      "2024-04-19 14:20:52 Step 325/1500: training loss=2.72\n",
      "2024-04-19 14:20:49 Step 324/1500: training loss=2.79\n",
      "2024-04-19 14:20:47 Step 323/1500: training loss=1.00\n",
      "2024-04-19 14:20:47 Step 322/1500: training loss=2.08\n",
      "2024-04-19 14:20:45 Step 321/1500: training loss=2.60\n",
      "2024-04-19 14:20:43 Step 320/1500: training loss=1.31\n",
      "2024-04-19 14:20:41 Step 319/1500: training loss=2.29\n",
      "2024-04-19 14:20:41 Step 318/1500: training loss=1.61\n",
      "2024-04-19 14:20:39 Step 317/1500: training loss=3.03\n",
      "2024-04-19 14:20:37 Step 316/1500: training loss=1.34\n",
      "2024-04-19 14:20:37 Step 315/1500: training loss=1.88\n",
      "2024-04-19 14:20:35 Step 314/1500: training loss=2.23\n",
      "2024-04-19 14:20:33 Step 313/1500: training loss=2.04\n",
      "2024-04-19 14:20:31 Step 312/1500: training loss=1.43\n",
      "2024-04-19 14:20:31 Step 311/1500: training loss=1.46\n",
      "2024-04-19 14:20:29 Step 310/1500: training loss=2.43\n",
      "2024-04-19 14:20:27 Step 309/1500: training loss=2.07\n",
      "2024-04-19 14:20:25 Step 308/1500: training loss=0.92\n",
      "2024-04-19 14:20:25 Step 307/1500: training loss=1.48\n",
      "2024-04-19 14:20:23 Step 306/1500: training loss=2.75\n",
      "2024-04-19 14:20:21 Step 305/1500: training loss=1.45\n",
      "2024-04-19 14:20:21 Step 304/1500: training loss=2.80\n",
      "2024-04-19 14:20:19 Step 303/1500: training loss=1.97\n",
      "2024-04-19 14:20:17 Step 302/1500: training loss=2.12\n",
      "2024-04-19 14:20:15 Step 301/1500: training loss=2.30\n",
      "2024-04-19 14:20:15 Step 300/1500: training loss=1.80\n",
      "2024-04-19 14:20:13 Step 299/1500: training loss=2.11\n",
      "2024-04-19 14:20:11 Step 298/1500: training loss=2.03\n",
      "2024-04-19 14:20:09 Step 297/1500: training loss=2.46\n",
      "2024-04-19 14:20:09 Step 296/1500: training loss=2.43\n",
      "2024-04-19 14:20:07 Step 295/1500: training loss=1.81\n",
      "2024-04-19 14:20:05 Step 294/1500: training loss=2.33\n",
      "2024-04-19 14:20:05 Step 293/1500: training loss=2.12\n",
      "2024-04-19 14:20:03 Step 292/1500: training loss=2.26\n",
      "2024-04-19 14:20:01 Step 291/1500: training loss=3.12\n",
      "2024-04-19 14:19:59 Step 290/1500: training loss=1.87\n",
      "2024-04-19 14:19:59 Step 289/1500: training loss=3.03\n",
      "2024-04-19 14:19:57 Step 288/1500: training loss=2.88\n",
      "2024-04-19 14:19:55 Step 287/1500: training loss=1.08\n",
      "2024-04-19 14:19:53 Step 286/1500: training loss=2.24\n",
      "2024-04-19 14:19:53 Step 285/1500: training loss=2.25\n",
      "2024-04-19 14:19:51 Step 284/1500: training loss=1.92\n",
      "2024-04-19 14:19:49 Step 283/1500: training loss=2.42\n",
      "2024-04-19 14:19:49 Step 282/1500: training loss=2.47\n",
      "2024-04-19 14:19:47 Step 281/1500: training loss=1.64\n",
      "2024-04-19 14:19:45 Step 280/1500: training loss=2.34\n",
      "2024-04-19 14:19:43 Step 279/1500: training loss=1.73\n",
      "2024-04-19 14:19:43 Step 278/1500: training loss=2.04\n",
      "2024-04-19 14:19:41 Step 277/1500: training loss=2.24\n",
      "2024-04-19 14:19:39 Step 276/1500: training loss=2.51\n",
      "2024-04-19 14:19:37 Step 275/1500: training loss=2.26\n",
      "2024-04-19 14:19:37 Step 274/1500: training loss=1.71\n",
      "2024-04-19 14:19:35 Step 273/1500: training loss=2.15\n",
      "2024-04-19 14:19:33 Step 272/1500: training loss=1.82\n",
      "2024-04-19 14:19:31 Step 271/1500: training loss=0.79\n",
      "2024-04-19 14:19:31 Step 270/1500: training loss=1.66\n",
      "2024-04-19 14:19:29 Step 269/1500: training loss=2.11\n",
      "2024-04-19 14:19:27 Step 268/1500: training loss=2.99\n",
      "2024-04-19 14:19:27 Step 267/1500: training loss=1.67\n",
      "2024-04-19 14:19:25 Step 266/1500: training loss=3.36\n",
      "2024-04-19 14:19:23 Step 265/1500: training loss=1.80\n",
      "2024-04-19 14:19:21 Step 264/1500: training loss=1.44\n",
      "2024-04-19 14:19:21 Step 263/1500: training loss=1.88\n",
      "2024-04-19 14:19:19 Step 262/1500: training loss=1.93\n",
      "2024-04-19 14:19:17 Step 261/1500: training loss=2.82\n",
      "2024-04-19 14:19:15 Step 260/1500: training loss=1.18\n",
      "2024-04-19 14:19:15 Step 259/1500: training loss=2.03\n",
      "2024-04-19 14:19:13 Step 258/1500: training loss=2.78\n",
      "2024-04-19 14:19:11 Step 257/1500: training loss=1.48\n",
      "2024-04-19 14:19:09 Step 256/1500: training loss=2.64\n",
      "2024-04-19 14:19:09 Step 255/1500: training loss=2.79\n",
      "2024-04-19 14:19:07 Step 254/1500: training loss=4.07\n",
      "2024-04-19 14:19:05 Step 253/1500: training loss=2.84\n",
      "2024-04-19 14:19:05 Step 252/1500: training loss=0.94\n",
      "2024-04-19 14:19:03 Step 251/1500: training loss=2.22\n",
      "2024-04-19 14:19:01 Step 250/1500: training loss=2.37\n",
      "2024-04-19 14:18:59 Step 249/1500: training loss=1.87\n",
      "2024-04-19 14:18:59 Step 248/1500: training loss=3.76\n",
      "2024-04-19 14:18:57 Step 247/1500: training loss=2.94\n",
      "2024-04-19 14:18:55 Step 246/1500: training loss=2.20\n",
      "2024-04-19 14:18:53 Step 245/1500: training loss=1.43\n",
      "2024-04-19 14:18:52 Step 244/1500: training loss=2.00\n",
      "2024-04-19 14:18:50 Step 243/1500: training loss=2.68\n",
      "2024-04-19 14:18:48 Step 242/1500: training loss=2.25\n",
      "2024-04-19 14:18:48 Step 241/1500: training loss=2.39\n",
      "2024-04-19 14:18:46 Step 240/1500: training loss=2.26\n",
      "2024-04-19 14:18:44 Step 239/1500: training loss=1.97\n",
      "2024-04-19 14:18:42 Step 238/1500: training loss=2.38\n",
      "2024-04-19 14:18:42 Step 237/1500: training loss=2.39\n",
      "2024-04-19 14:18:40 Step 236/1500: training loss=1.50\n",
      "2024-04-19 14:18:38 Step 235/1500: training loss=3.71\n",
      "2024-04-19 14:18:36 Step 234/1500: training loss=2.13\n",
      "2024-04-19 14:18:36 Step 233/1500: training loss=1.85\n",
      "2024-04-19 14:18:34 Step 232/1500: training loss=1.85\n",
      "2024-04-19 14:18:32 Step 231/1500: training loss=1.57\n",
      "2024-04-19 14:18:30 Step 230/1500: training loss=1.80\n",
      "2024-04-19 14:18:30 Step 229/1500: training loss=1.75\n",
      "2024-04-19 14:18:28 Step 228/1500: training loss=1.99\n",
      "2024-04-19 14:18:26 Step 227/1500: training loss=2.00\n",
      "2024-04-19 14:18:26 Step 226/1500: training loss=2.20\n",
      "2024-04-19 14:18:24 Step 225/1500: training loss=2.34\n",
      "2024-04-19 14:18:22 Step 224/1500: training loss=2.92\n",
      "2024-04-19 14:18:20 Step 223/1500: training loss=2.26\n",
      "2024-04-19 14:18:20 Step 222/1500: training loss=3.28\n",
      "2024-04-19 14:18:18 Step 221/1500: training loss=2.72\n",
      "2024-04-19 14:18:16 Step 220/1500: training loss=1.87\n",
      "2024-04-19 14:18:14 Step 219/1500: training loss=2.65\n",
      "2024-04-19 14:18:14 Step 218/1500: training loss=2.23\n",
      "2024-04-19 14:18:12 Step 217/1500: training loss=2.50\n",
      "2024-04-19 14:18:10 Step 216/1500: training loss=1.66\n",
      "2024-04-19 14:18:10 Step 215/1500: training loss=2.26\n",
      "2024-04-19 14:18:08 Step 214/1500: training loss=3.13\n",
      "2024-04-19 14:18:06 Step 213/1500: training loss=2.12\n",
      "2024-04-19 14:18:04 Step 212/1500: training loss=2.72\n",
      "2024-04-19 14:18:04 Step 211/1500: training loss=1.69\n",
      "2024-04-19 14:18:02 Step 210/1500: training loss=2.80\n",
      "2024-04-19 14:18:00 Step 209/1500: training loss=2.46\n",
      "2024-04-19 14:17:58 Step 208/1500: training loss=2.33\n",
      "2024-04-19 14:17:58 Step 207/1500: training loss=2.91\n",
      "2024-04-19 14:17:56 Step 206/1500: training loss=1.61\n",
      "2024-04-19 14:17:54 Step 205/1500: training loss=2.22\n",
      "2024-04-19 14:17:52 Step 204/1500: training loss=0.66\n",
      "2024-04-19 14:17:52 Step 203/1500: training loss=2.04\n",
      "2024-04-19 14:17:50 Step 202/1500: training loss=2.68\n",
      "2024-04-19 14:17:48 Step 201/1500: training loss=2.44\n",
      "2024-04-19 14:17:46 Step 200/1500: training loss=0.97\n",
      "2024-04-19 14:17:46 Step 199/1500: training loss=2.13\n",
      "2024-04-19 14:17:44 Step 198/1500: training loss=2.03\n",
      "2024-04-19 14:17:42 Step 197/1500: training loss=1.98\n",
      "2024-04-19 14:17:42 Step 196/1500: training loss=1.07\n",
      "2024-04-19 14:17:40 Step 195/1500: training loss=3.18\n",
      "2024-04-19 14:17:38 Step 194/1500: training loss=3.40\n",
      "2024-04-19 14:17:36 Step 193/1500: training loss=1.45\n",
      "2024-04-19 14:17:36 Step 192/1500: training loss=2.42\n",
      "2024-04-19 14:17:34 Step 191/1500: training loss=1.79\n",
      "2024-04-19 14:17:32 Step 190/1500: training loss=2.44\n",
      "2024-04-19 14:17:30 Step 189/1500: training loss=2.72\n",
      "2024-04-19 14:17:30 Step 188/1500: training loss=2.70\n",
      "2024-04-19 14:17:28 Step 187/1500: training loss=1.99\n",
      "2024-04-19 14:17:26 Step 186/1500: training loss=2.35\n",
      "2024-04-19 14:17:24 Step 185/1500: training loss=1.34\n",
      "2024-04-19 14:17:24 Step 184/1500: training loss=1.81\n",
      "2024-04-19 14:17:22 Step 183/1500: training loss=3.09\n",
      "2024-04-19 14:17:20 Step 182/1500: training loss=2.08\n",
      "2024-04-19 14:17:20 Step 181/1500: training loss=2.01\n",
      "2024-04-19 14:17:18 Step 180/1500: training loss=2.77\n",
      "2024-04-19 14:17:16 Step 179/1500: training loss=2.19\n",
      "2024-04-19 14:17:14 Step 178/1500: training loss=1.75\n",
      "2024-04-19 14:17:14 Step 177/1500: training loss=2.03\n",
      "2024-04-19 14:17:12 Step 176/1500: training loss=1.68\n",
      "2024-04-19 14:17:10 Step 175/1500: training loss=3.04\n",
      "2024-04-19 14:17:08 Step 174/1500: training loss=1.85\n",
      "2024-04-19 14:17:08 Step 173/1500: training loss=0.90\n",
      "2024-04-19 14:17:06 Step 172/1500: training loss=3.78\n",
      "2024-04-19 14:17:04 Step 171/1500: training loss=1.98\n",
      "2024-04-19 14:17:04 Step 170/1500: training loss=1.93\n",
      "2024-04-19 14:17:02 Step 169/1500: training loss=1.95\n",
      "2024-04-19 14:17:00 Step 168/1500: training loss=2.74\n",
      "2024-04-19 14:16:58 Step 167/1500: training loss=2.23\n",
      "2024-04-19 14:16:58 Step 166/1500: training loss=1.24\n",
      "2024-04-19 14:16:56 Step 165/1500: training loss=2.35\n",
      "2024-04-19 14:16:54 Step 164/1500: training loss=2.00\n",
      "2024-04-19 14:16:52 Step 163/1500: training loss=1.21\n",
      "2024-04-19 14:16:52 Step 162/1500: training loss=1.55\n",
      "2024-04-19 14:16:50 Step 161/1500: training loss=3.74\n",
      "2024-04-19 14:16:48 Step 160/1500: training loss=2.20\n",
      "2024-04-19 14:16:48 Step 159/1500: training loss=0.58\n",
      "2024-04-19 14:16:45 Step 158/1500: training loss=2.48\n",
      "2024-04-19 14:16:43 Step 157/1500: training loss=1.78\n",
      "2024-04-19 14:16:41 Step 156/1500: training loss=2.65\n",
      "2024-04-19 14:16:41 Step 155/1500: training loss=2.14\n",
      "2024-04-19 14:16:39 Step 154/1500: training loss=3.52\n",
      "2024-04-19 14:16:37 Step 153/1500: training loss=2.17\n",
      "2024-04-19 14:16:35 Step 152/1500: training loss=1.85\n",
      "2024-04-19 14:16:35 Step 151/1500: training loss=1.76\n",
      "2024-04-19 14:16:33 Step 150/1500: training loss=2.63\n",
      "2024-04-19 14:16:31 Step 149/1500: training loss=2.22\n",
      "2024-04-19 14:16:31 Step 148/1500: training loss=4.21\n",
      "2024-04-19 14:16:29 Step 147/1500: training loss=2.49\n",
      "2024-04-19 14:16:27 Step 146/1500: training loss=2.52\n",
      "2024-04-19 14:16:25 Step 145/1500: training loss=1.94\n",
      "2024-04-19 14:16:25 Step 144/1500: training loss=1.56\n",
      "2024-04-19 14:16:23 Step 143/1500: training loss=2.04\n",
      "2024-04-19 14:16:21 Step 142/1500: training loss=1.58\n",
      "2024-04-19 14:16:19 Step 141/1500: training loss=1.80\n",
      "2024-04-19 14:16:19 Step 140/1500: training loss=2.43\n",
      "2024-04-19 14:16:17 Step 139/1500: training loss=2.66\n",
      "2024-04-19 14:16:15 Step 138/1500: training loss=1.90\n",
      "2024-04-19 14:16:15 Step 137/1500: training loss=2.67\n",
      "2024-04-19 14:16:13 Step 136/1500: training loss=3.01\n",
      "2024-04-19 14:16:11 Step 135/1500: training loss=1.71\n",
      "2024-04-19 14:16:09 Step 134/1500: training loss=2.11\n",
      "2024-04-19 14:16:09 Step 133/1500: training loss=2.36\n",
      "2024-04-19 14:16:07 Step 132/1500: training loss=2.45\n",
      "2024-04-19 14:16:05 Step 131/1500: training loss=1.56\n",
      "2024-04-19 14:16:05 Step 130/1500: training loss=1.22\n",
      "2024-04-19 14:16:03 Step 129/1500: training loss=2.96\n",
      "2024-04-19 14:16:01 Step 128/1500: training loss=2.19\n",
      "2024-04-19 14:15:59 Step 127/1500: training loss=2.19\n",
      "2024-04-19 14:15:59 Step 126/1500: training loss=1.24\n",
      "2024-04-19 14:15:57 Step 125/1500: training loss=2.21\n",
      "2024-04-19 14:15:55 Step 124/1500: training loss=2.37\n",
      "2024-04-19 14:15:53 Step 123/1500: training loss=2.98\n",
      "2024-04-19 14:15:53 Step 122/1500: training loss=2.72\n",
      "2024-04-19 14:15:51 Step 121/1500: training loss=1.67\n",
      "2024-04-19 14:15:49 Step 120/1500: training loss=2.05\n",
      "2024-04-19 14:15:49 Step 119/1500: training loss=2.28\n",
      "2024-04-19 14:15:47 Step 118/1500: training loss=2.72\n",
      "2024-04-19 14:15:45 Step 117/1500: training loss=2.74\n",
      "2024-04-19 14:15:43 Step 116/1500: training loss=3.37\n",
      "2024-04-19 14:15:43 Step 115/1500: training loss=2.06\n",
      "2024-04-19 14:15:41 Step 114/1500: training loss=1.09\n",
      "2024-04-19 14:15:39 Step 113/1500: training loss=1.80\n",
      "2024-04-19 14:15:39 Step 112/1500: training loss=2.01\n",
      "2024-04-19 14:15:37 Step 111/1500: training loss=1.86\n",
      "2024-04-19 14:15:35 Step 110/1500: training loss=2.27\n",
      "2024-04-19 14:15:33 Step 109/1500: training loss=1.89\n",
      "2024-04-19 14:15:33 Step 108/1500: training loss=2.22\n",
      "2024-04-19 14:15:31 Step 107/1500: training loss=2.61\n",
      "2024-04-19 14:15:29 Step 106/1500: training loss=1.75\n",
      "2024-04-19 14:15:29 Step 105/1500: training loss=1.90\n",
      "2024-04-19 14:15:27 Step 104/1500: training loss=2.27\n",
      "2024-04-19 14:15:25 Step 103/1500: training loss=2.72\n",
      "2024-04-19 14:15:23 Step 102/1500: training loss=1.68\n",
      "2024-04-19 14:15:23 Step 101/1500: training loss=2.06\n",
      "2024-04-19 14:15:21 Step 100/1500: training loss=2.74\n",
      "2024-04-19 14:15:19 Step 99/1500: training loss=1.40\n",
      "2024-04-19 14:15:17 Step 98/1500: training loss=4.44\n",
      "2024-04-19 14:15:17 Step 97/1500: training loss=1.83\n",
      "2024-04-19 14:15:15 Step 96/1500: training loss=1.51\n",
      "2024-04-19 14:15:13 Step 95/1500: training loss=2.66\n",
      "2024-04-19 14:15:13 Step 94/1500: training loss=2.26\n",
      "2024-04-19 14:15:11 Step 93/1500: training loss=1.58\n",
      "2024-04-19 14:15:09 Step 92/1500: training loss=1.70\n",
      "2024-04-19 14:15:07 Step 91/1500: training loss=1.80\n",
      "2024-04-19 14:15:07 Step 90/1500: training loss=2.76\n",
      "2024-04-19 14:15:05 Step 89/1500: training loss=2.24\n",
      "2024-04-19 14:15:03 Step 88/1500: training loss=1.66\n",
      "2024-04-19 14:15:03 Step 87/1500: training loss=2.36\n",
      "2024-04-19 14:15:01 Step 86/1500: training loss=2.63\n",
      "2024-04-19 14:14:59 Step 85/1500: training loss=1.61\n",
      "2024-04-19 14:14:57 Step 84/1500: training loss=2.27\n",
      "2024-04-19 14:14:57 Step 83/1500: training loss=1.41\n",
      "2024-04-19 14:14:55 Step 82/1500: training loss=1.60\n",
      "2024-04-19 14:14:52 Step 81/1500: training loss=1.81\n",
      "2024-04-19 14:14:50 Step 80/1500: training loss=3.06\n",
      "2024-04-19 14:14:50 Step 79/1500: training loss=1.95\n",
      "2024-04-19 14:14:48 Step 78/1500: training loss=2.53\n",
      "2024-04-19 14:14:46 Step 77/1500: training loss=2.39\n",
      "2024-04-19 14:14:46 Step 76/1500: training loss=3.18\n",
      "2024-04-19 14:14:44 Step 75/1500: training loss=2.45\n",
      "2024-04-19 14:14:42 Step 74/1500: training loss=2.36\n",
      "2024-04-19 14:14:40 Step 73/1500: training loss=1.72\n",
      "2024-04-19 14:14:40 Step 72/1500: training loss=2.53\n",
      "2024-04-19 14:14:38 Step 71/1500: training loss=3.12\n",
      "2024-04-19 14:14:36 Step 70/1500: training loss=1.50\n",
      "2024-04-19 14:14:36 Step 69/1500: training loss=3.02\n",
      "2024-04-19 14:14:34 Step 68/1500: training loss=2.20\n",
      "2024-04-19 14:14:32 Step 67/1500: training loss=6.09\n",
      "2024-04-19 14:14:30 Step 66/1500: training loss=2.54\n",
      "2024-04-19 14:14:30 Step 65/1500: training loss=1.42\n",
      "2024-04-19 14:14:28 Step 64/1500: training loss=2.22\n",
      "2024-04-19 14:14:26 Step 63/1500: training loss=4.02\n",
      "2024-04-19 14:14:26 Step 62/1500: training loss=2.87\n",
      "2024-04-19 14:14:24 Step 61/1500: training loss=2.06\n",
      "2024-04-19 14:14:22 Step 60/1500: training loss=1.70\n",
      "2024-04-19 14:14:20 Step 59/1500: training loss=2.03\n",
      "2024-04-19 14:14:20 Step 58/1500: training loss=1.63\n",
      "2024-04-19 14:14:18 Step 57/1500: training loss=2.53\n",
      "2024-04-19 14:14:16 Step 56/1500: training loss=2.94\n",
      "2024-04-19 14:14:14 Step 55/1500: training loss=2.25\n",
      "2024-04-19 14:14:14 Step 54/1500: training loss=2.39\n",
      "2024-04-19 14:14:12 Step 53/1500: training loss=2.22\n",
      "2024-04-19 14:14:10 Step 52/1500: training loss=2.82\n",
      "2024-04-19 14:14:10 Step 51/1500: training loss=2.61\n",
      "2024-04-19 14:14:08 Step 50/1500: training loss=2.07\n",
      "2024-04-19 14:14:06 Step 49/1500: training loss=2.38\n",
      "2024-04-19 14:14:04 Step 48/1500: training loss=2.14\n",
      "2024-04-19 14:14:04 Step 47/1500: training loss=2.00\n",
      "2024-04-19 14:14:02 Step 46/1500: training loss=2.24\n",
      "2024-04-19 14:14:00 Step 45/1500: training loss=2.28\n",
      "2024-04-19 14:13:58 Step 44/1500: training loss=2.90\n",
      "2024-04-19 14:13:58 Step 43/1500: training loss=2.41\n",
      "2024-04-19 14:13:56 Step 42/1500: training loss=2.07\n",
      "2024-04-19 14:13:54 Step 41/1500: training loss=3.12\n",
      "2024-04-19 14:13:54 Step 40/1500: training loss=3.05\n",
      "2024-04-19 14:13:52 Step 39/1500: training loss=1.79\n",
      "2024-04-19 14:13:50 Step 38/1500: training loss=2.44\n",
      "2024-04-19 14:13:48 Step 37/1500: training loss=2.37\n",
      "2024-04-19 14:13:48 Step 36/1500: training loss=3.17\n",
      "2024-04-19 14:13:46 Step 35/1500: training loss=3.19\n",
      "2024-04-19 14:13:44 Step 34/1500: training loss=2.56\n",
      "2024-04-19 14:13:42 Step 33/1500: training loss=3.85\n",
      "2024-04-19 14:13:42 Step 32/1500: training loss=5.15\n",
      "2024-04-19 14:13:40 Step 31/1500: training loss=2.51\n",
      "2024-04-19 14:13:38 Step 30/1500: training loss=3.54\n",
      "2024-04-19 14:13:36 Step 29/1500: training loss=3.49\n",
      "2024-04-19 14:13:36 Step 28/1500: training loss=2.60\n",
      "2024-04-19 14:13:34 Step 27/1500: training loss=2.13\n",
      "2024-04-19 14:13:32 Step 26/1500: training loss=1.80\n",
      "2024-04-19 14:13:30 Step 25/1500: training loss=3.26\n",
      "2024-04-19 14:13:30 Step 24/1500: training loss=3.48\n",
      "2024-04-19 14:13:28 Step 23/1500: training loss=4.33\n",
      "2024-04-19 14:13:26 Step 22/1500: training loss=4.28\n",
      "2024-04-19 14:13:24 Step 21/1500: training loss=3.53\n",
      "2024-04-19 14:13:24 Step 20/1500: training loss=3.19\n",
      "2024-04-19 14:13:22 Step 19/1500: training loss=4.67\n",
      "2024-04-19 14:13:20 Step 18/1500: training loss=5.04\n",
      "2024-04-19 14:13:20 Step 17/1500: training loss=2.39\n",
      "2024-04-19 14:13:18 Step 16/1500: training loss=4.85\n",
      "2024-04-19 14:13:16 Step 15/1500: training loss=3.91\n",
      "2024-04-19 14:13:14 Step 14/1500: training loss=5.57\n",
      "2024-04-19 14:13:10 Step 13/1500: training loss=7.31\n",
      "2024-04-19 14:13:08 Step 12/1500: training loss=2.73\n",
      "2024-04-19 14:13:06 Step 11/1500: training loss=2.23\n",
      "2024-04-19 14:13:04 Step 10/1500: training loss=2.80\n",
      "2024-04-19 14:13:00 Step 9/1500: training loss=2.18\n",
      "2024-04-19 14:12:57 Step 8/1500: training loss=3.62\n",
      "2024-04-19 14:12:55 Step 7/1500: training loss=4.44\n",
      "2024-04-19 14:12:53 Step 6/1500: training loss=2.66\n",
      "2024-04-19 14:12:49 Step 5/1500: training loss=7.63\n",
      "2024-04-19 14:12:47 Step 4/1500: training loss=3.86\n",
      "2024-04-19 14:12:45 Step 3/1500: training loss=4.85\n",
      "2024-04-19 14:12:43 Step 2/1500: training loss=4.79\n",
      "2024-04-19 14:12:41 Step 1/1500: training loss=3.81\n",
      "2024-04-19 14:11:31 Fine-tuning job started\n",
      "2024-04-19 14:11:30 Files validated, moving job to queued state\n",
      "2024-04-19 14:10:47 Validating training file: file-pqN0u7lBPpI82Bv4udlgpejD\n",
      "2024-04-19 14:10:47 Created fine-tuning job: ftjob-i3343i8eFZ1lbBYoWKDJvL4w\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "import datetime\n",
    "\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    status = client.fine_tuning.jobs.retrieve(job_id).status\n",
    "    print(f\"Stream interrupted. Job is still {status}.\")\n",
    "    return\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "events = client.fine_tuning.jobs.list_events(fine_tuning_job_id='ftjob-i3343i8eFZ1lbBYoWKDJvL4w')\n",
    "try:\n",
    "    for event in events:\n",
    "        print(\n",
    "            f'{datetime.datetime.fromtimestamp(event.created_at)} {event.message}'\n",
    "        )\n",
    "except Exception:\n",
    "    print(\"Stream interrupted (client disconnected).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job not in terminal status: running. Waiting.\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: running\n",
      "Status: succeeded\n",
      "Checking other finetune jobs in the subscription.\n",
      "Found 1 finetune jobs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "status = client.fine_tuning.jobs.retrieve(job_id).status\n",
    "if status not in [\"succeeded\", \"failed\"]:\n",
    "    print(f\"Job not in terminal status: {status}. Waiting.\")\n",
    "    while status not in [\"succeeded\", \"failed\"]:\n",
    "        time.sleep(2)\n",
    "        status = client.fine_tuning.jobs.retrieve(job_id).status\n",
    "        print(f\"Status: {status}\")\n",
    "else:\n",
    "    print(f\"Finetune job {job_id} finished with status: {status}\")\n",
    "print(\"Checking other finetune jobs in the subscription.\")\n",
    "result = client.fine_tuning.jobs.list()\n",
    "print(f\"Found {len(result.data)} finetune jobs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0125:personal::9FcMkILC\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = result.data[0].fine_tuned_model\n",
    "print(fine_tuned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Marv is a factual chatbot and reddit expert who likes to answer with bullets\"\n",
    "user_question = \"Give me the dumbest thing you've ever done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tried to microwave and egg to see if I could make a hard boiled egg without boiling it. It exploded. And the left front burner. '\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"ft:gpt-3.5-turbo-0125:personal::9FcMkILC\",\n",
    "    messages = [\n",
    "        {'role':'system', \"content\": system_prompt},\n",
    "        {'role':'user', \"content\": user_question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
